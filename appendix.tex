\chapter{EM-CCD camera calibration}
\section{Andor Basic code listing for automatic image acquisition}
\label{sec:basic-acquisition}
\definecolor{light-gray}{gray}{0.95}

\lstdefinestyle{myframe}{
  basicstyle=\LSTfont,
  % basicstyle=\footnotesize\ttfamily,
  rulesepcolor=\color{gray} ,
  rulecolor = \color{black},
  frame = single,
  % framerule = 0pt,
 % backgroundcolor =\color{light-gray}, 
  fontadjust=true,
  breaklines = true,
  showstringspaces=false,
  commentstyle=\itshape,
}
\lstdefinestyle{mymaxima}{
  language=C,
  title={Maxima},
  style=myframe
}
\lstdefinestyle{myclang}{
  language=C,
  title={C language},
  style=myframe
}
\lstdefinestyle{myfortran}{
  language=Fortran,
  style=myframe
}
\lstdefinestyle{mymatlab}{
  language=Matlab,
  title={Matlab},
  style=myframe
}
\lstdefinestyle{mylisp}{
  language=Lisp,
  title={Common Lisp},
  style=myframe
}
\lstdefinestyle{mybasic}{
  title={Andor Basic},
  language=[Visual]Basic,
  style=myframe
}
\lstdefinestyle{mypython}{
  language=Python,
  title={Python},
  showstringspaces=false,
  tabsize=4,
  basicstyle=\ttfamily,
  morekeywords={models, lambda, forms},
  frame = single,
  breaklines = true,
  style=myframe
}

FIXME translate:

Der folgende Quellcode ermoeglicht es mit moeglichst wenig
Nutzerinteraktion Kalibrationsdaten fuer eine Andor EMCCD Kamera
aufzunehmen.

Das Programm ist in einem Basicdialekt geschrieben und kann in der
Software Andor Solis ausgefuehrt werden. Vor der Benutzung sollte ein
Bild auf der Kamera erzeugt werden (z.B. mit einem defokussiertem
Fluoreszenten Sample). Die Daten koennen mit in Matlab/DIPimage mit
der Funktion \verb!cal_readnoise! oder, falls Matlab keine Option ist,
mit dem Pythonskript aus dem naechsten Abschnitt ausgewertet
werden.

Die Auswertung erlaubt es spaeter andere Kamerabilder, die mit
gleichen Einstellungen --- pre-amp Verstaerkung, EM-gain Verstaerkung,
Temperatur, vertical shift speed, readout rate --- aufgenommen werden,
in der Geraeteunabhaengigen Einheit von detektierten Photoelektronen
anzugeben. Es ist weiterhin moeglich, den tatsaechlich applizierten
EM-gain oder den excess noise Faktor zu messen. Genauere ausfuehrungen
dazu befinden sich im Haupttext im Abschnitt ref:ccd-intro.

Die Kamera, die ich bei der Entwicklung dieses Programms eingesetzt
habe verfuegt ueber einen internen mechanischen Shutter. Das ist
nuetzlich weil es wichtig ist, fuer jede Einstellung auch mindestens
eine Dunkelaufnahme zu machen. Aus dieser wird letztendlich das
Ausleserauschen berechnet.

Die hier abgedruckten Listings befinden sich hintereinander in einer
Datei die auch unter FIXME github link abrufbar ist.

Ich beschreibe nun den Code. Angenommen das Mikroskop ist auf ein den
Anforderungen fuer die Kalibrierung genuegendes Sample eingestellt und
liefert einen kontinuierlichen Photonenfluss. Mit dem Programm sollen
fuer verschiedene Einstellungen des EM-gain Bilder aufgenommen
werden. Da der Photonenfluss konstant bleibt, die Verstaerkung jedoch
stark variiert, habe ich vor jeder Messreihe eine Aufnahme mit
$\unit[10]{\mu s}$ kurzer Belichtungszeit aufgenommen und aus dem
darin befindlichen Maximum die Belichtungszeit ausgerechnet um bei
gegebenen Gain \unit[10000]{ADU} im Maximum zu erhalten. Die folgende
Funktion macht diese Probemessung und gibt die Belichtungszeit
zurueck:
\begin{lstlisting}[style=mybasic]
function ~GetSaturatingExposure()
        SetKineticNumber(1)
        exp=.01
        SetExposureTime(exp)
        run()
        m=maximum(#0,1,512)
        GetSaturatingExposure=exp*10000/(m-100) % 100 is the background in ADU
        CloseWindow(#0)
return
\end{lstlisting}
Der folgende Code-Abschnitt stellt das konventionelle readout Register
ein (ein Register ohne EM-gain), nimmt 20 Bilder ohne und dann mit
Licht auf und speichert diese als eine TIF-Bilddatei.
\begin{lstlisting}[style=mybasic]
name$ = "C:\Users\work\Desktop\martin\20111111\scan-em3\ixon_"
print("start")

SetOutputAmp(1)
print("conv_start")
exp= ~GetSaturatingExposure()
print(exp)
SetExposureTime(exp)
SetKineticNumber(20)
SetShutter(0,1)
run()
save(#0,name$ + "conv1_dark.sif")
ExportTiff(#0, name$ + "conv1_dark.tif", 1, 1, 0, 0)
CloseWindow(#0)
CloseWindow(#1)

SetShutter(1,1)
run()
save(#0,name$ + "conv1_bright.sif")
ExportTiff(#0, name$ + "conv1_bright.tif", 1, 1, 0, 0)
CloseWindow(#0)
CloseWindow(#1)
\end{lstlisting}
Die folgende Schleife macht entsprechende Aufnahmen fuer verschiedene
EM-gains. Bei der Auswertung hat sich herausgestellt, dass
\verb!SetGain! eine gewisse Zeit zur Stabilisierung bedarf. Immerhin
handelt es sich um relativ hohe Spannungen. Ich habe die entsprechende
Zeile mit einem Kommentar versehen.
\begin{lstlisting}[style=mybasic]
SetOutputAmp(0)
SetShutter(1,1)
for i = 40 to 300 step 10
        SetGain(i)
        % here should be a 3s wait
        exp=~GetSaturatingExposure()
        print(exp)
        SetExposureTime(exp)
        SetKineticNumber(20)
        SetShutter(0,1)
        run()
        save(#0,name$ + str$(i) + "_dark.sif")
        ExportTiff(#0, name$ + str$(i) + "_dark.tif", 1, 1, 0, 0)
        CloseWindow(#0)
        CloseWindow(#1)
        SetShutter(1,1)
        run()
        save(#0,name$ + str$(i) + "_bright.sif")
        ExportTiff(#0, name$ + str$(i) + "_bright.tif", 1, 1, 0, 0)
        CloseWindow(#0)
        CloseWindow(#1)
next
\end{lstlisting}
Abschliessend habe ich eine weitere Aufnahme mit dem konventionellem
readout Register gemacht. Leider hat sich dabei gezeigt, dass das
sample waehrend des experiments moeglicherweise etwas gebleicht
wurde. Daher waere es bei einer Wiederholung besser, eine LED
Lichtquelle einzusetzen oder konventionelle Aufnahmen in die
EM-Messungen einzustreuen, um derartige Effekte kompensieren zu
koennen.
\begin{lstlisting}[style=mybasic]
SetOutputAmp(1)
print("conv_end")
exp= ~GetSaturatingExposure()
print(exp)
SetExposureTime(exp)
SetKineticNumber(20)
SetShutter(0,1)
run()
save(#0,name$ + "conv2_dark.sif")
ExportTiff(#0, name$ + "conv2_dark.tif", 1, 1, 0, 0)
CloseWindow(#0)
CloseWindow(#1)
        
SetShutter(1,1)
run()
save(#0,name$ + "conv2_bright.sif")
ExportTiff(#0, name$ + "conv2 _bright.tif", 1, 1, 0, 0)
CloseWindow(#0)
CloseWindow(#1)
\end{lstlisting}
Die Tabelle \ref{tab:ixon-table} zeigt die ausgewertenden Daten fuer
eine solche Messreihe.

\comment{
$
}

\begin{table}[!htbp]
  \centering
%  \begin{tabular}{|l|l|l|l|l|l|l|l|}
  \begin{tabular}{r l l r  l r l}
\hline
$\textsf{gain}_\textrm{software}$ & $1/(M\cdot M_\textrm{pre})$ & $N_r$ & $N_{(M)}/(W\times H)$ &  \textsf{exposure} & $N_{(M)}'/(W\times H)$ & $1/F_n$ \\
 & [e/ADU] & [e/px] & [e/px] & [ADU] & [s] & [e/(px s)]  \\
\hline
conv1 & 1.3165 & 7.189 & 3008.66      & 0.2016 & 14923 & 0.981 \\
50 & 0.1160 & 0.486 & 260.05 & 0.0289 & 8995 & 0.591 \\
60 & 0.0984 & 0.406 & 225.46 & 0.0249 & 9054 & 0.595 \\
70 & 0.0841 & 0.349 & 190.52 & 0.0212 & 8983 & 0.591 \\
80 & 0.0729 & 0.305 & 165.24 & 0.0186 & 8907 & 0.586 \\
90 & 0.0680 & 0.288 & 150.54 & 0.0161 & 9368 & 0.616 \\
100 & 0.0611 & 0.262 & 128.47 & 0.0136 & 9427 & 0.620 \\
110 & 0.0550 & 0.241 & 121.11 & 0.0129 & 9409 & 0.619 \\
120 & 0.0510 & 0.228 & 113.71 & 0.0120 & 9498 & 0.624 \\
130 & 0.0465 & 0.211 & 106.66 & 0.0112 & 9541 & 0.627 \\
140 & 0.0433 & 0.201 & 96.95 & 0.0101 & 9564 & 0.629 \\
150 & 0.0405 & 0.192 & 89.68 & 0.0093 & 9671 & 0.636 \\
160 & 0.0380 & 0.183 & 87.24 & 0.0090 & 9656 & 0.635 \\
170 & 0.0359 & 0.175 & 81.56 & 0.0084 & 9739 & 0.640 \\
180 & 0.0339 & 0.169 & 79.80 & 0.0081 & 9863 & 0.648 \\
190 & 0.0321 & 0.163 & 74.00 & 0.0075 & 9806 & 0.645 \\
200 & 0.0305 & 0.158 & 72.57 & 0.0073 & 9878 & 0.649 \\
210 & 0.0292 & 0.155 & 69.44 & 0.0070 & 9944 & 0.654 \\
220 & 0.0280 & 0.150 & 67.69 & 0.0068 & 9971 & 0.656 \\
230 & 0.0268 & 0.147 & 65.63 & 0.0065 & 10057 & 0.661 \\
240 & 0.0257 & 0.188 & 63.90 & 0.0063 & 10131 & 0.666 \\
250 & 0.0244 & 0.140 & 62.52 & 0.0062 & 10026 & 0.659 \\
260 & 0.0237 & 0.137 & 62.86 & 0.0062 & 10078 & 0.663 \\
270 & 0.0229 & 0.135 & 63.17 & 0.0062 & 10130 & 0.666 \\
280 & 0.0221 & 0.133 & 63.64 & 0.0062 & 10204 & 0.671 \\
290 & 0.0214 & 0.130 & 63.38 & 0.0062 & 10162 & 0.668 \\
300 & 0.0205 & 0.128 & 63.20 & 0.0062 & 10133 & 0.666 \\
conv2 & 1.5953 & 8.768 & 8198.86 & 0.5291 & 15496 & 1.019 \\
\hline
\end{tabular}
%  \includegraphics[width=12cm]{../app_cam/ixon3}
\caption{Comparison of read noise for different EM-gain settings
  (first column) of the Andor IXon3. $W$ and $H$ are the size of the sensor (in pixels). The value $N_{(M)}'$
  estimates the number of photoelectrons the detector would have
  seen with \unit[1]{s} integration time and is used to calculate
  the excess noise factor in the last column. In EM-mode the fastest
  readout speed was used \unit[10]{MHz} with vertical shift speed of
  \unit[1.7]{$\mu$s}.}
  \label{tab:ixon-table}
\end{table}

\newpage

\section{Python code listing for the read noise evaluation}
\label{sec:python-readnoise-eval}
Im folgenden Abschnitt stelle ich eine Pythonimplementierung zur
Auswertung der mit dem im vorhergehenden Abschnitt beschriebenen
Programm aufgenommenen Daten vor.  \figref{fig:ixon} zeigt zwei
Auswertungen deren zugrundeliegende Daten fuer zwei verschiedene
Parameter fuer eine Andor IXon3 EM-CCD aufgenommen wurden.
\begin{figure}[htbp]
  \centering
  \pdfinput{17cm}{ixon_conv1}
  \pdfinput{17cm}{ixon_300}
  \caption{Readnoise evaluation using the Python code{\bf top:}
    Conventional readout of an Andor IXon3 camera. {\bf bottom:}
    readout with an EM-gain setting of 300 on the same camera with
    identical sample. {\bf left:} 2D histogram of per pixel variances
    against binned intensities. {\bf middle:} variance of 20 dark
    images. {\bf right:} mean of 20 dark images.}
  \label{fig:ixon}
\end{figure}
  
Der folgende Code laedt einige Pythonpackete. Im wesentlichen benutze
ich \verb!numpy! in \verb!pylab! fuer die Datenauswertung und
\verb!matplotlib! fuer die visualisierung der Resultate.
\begin{lstlisting}[style=mypython]
#!/usr/bin/env python
# usage:   ti.py DIRECTORY CAMERA_NAME EM_GAIN
# example: ti.py /media/backup/andor-ultra-ixon/martin/20111111/scan-em3/ ultra 2700
import sys
import os
import matplotlib
matplotlib.use('Agg')
from pylab import *
from libtiff import TIFFfile, TIFFimage
from scipy import stats
seterr(divide='ignore')
\end{lstlisting}
Beim start des Programmes uebergebe ich Kommadozeilenparameter, um
anzugeben, welche Daten geladen werden sollen. Der folgende Code liest
dann die entsprechenden Bilder mit und ohne Beleuchtung.
\begin{lstlisting}[style=mypython]
folder = sys.argv[1]
cam = sys.argv[2]
gain = sys.argv[3]

def readpics(gain,cam='ixon_',isdark=False):
    print 'loading ', os.path.join(folder,cam) + '_' + gain + '_bright.tif'
    fg=TIFFfile(os.path.join(folder,cam) + '_' + gain + '_bright.tif')
    bright,bright_names=fg.get_samples()
    bg=TIFFfile(os.path.join(folder,cam) + '_' + gain + '_dark.tif')    
    dark,dark_names=bg.get_samples()
    return (bright[0],dark[0])

(f,b) = readpics(gain=gain,cam=cam)
\end{lstlisting}
Mit dem folgenden Code erzeuge ich ein zweidimensionales Histogramm
mit 64 Varianzbins und 128 Intensitaetsbins:
\begin{lstlisting}[style=mypython]
ny,nx=64,128
H,y,x=histogram2d(v.flatten(),i.flatten(),bins=[ny,nx],
                  range=[[0,v.max()],[0,i.max()]])
extent = [x[0], x[-1], y[0], y[-1]] 

fig=figure(figsize=(24, 8),dpi=300)
hold(False)
title('bal')
subplot(1,3,1)
imshow(log(H), extent=extent,
           aspect='auto', interpolation='none',origin='lower')
hold(True)
\end{lstlisting}
Das Histogram ist nicht unbedingt notwendig, fuer die Auswertung,
erlaubt aber eine schnelle Einschaetzung der gemessenen Daten, d.h. ob
genuegend Messwerte fuer alle Intensitaeten existieren und der Sensor
nicht ueberbelichtet wurde. 

Der wichtigste Teil der Auswertung erfolgt im folgenden
Codesegement. Hier akkumuliere ich in den Variablen \verb!acc! und
\verb!accn! die Daten um die Mittelwerte der Varianzen fuer alle
Intensitaeten zu bestimmen.
\begin{lstlisting}[style=mypython]
acc=zeros(x.shape,dtype=float64)
accn=zeros(x.shape,dtype=int64)
s=nx/i.max()
for ii,vv in nditer([i,v]):
    p=round(ii*s)
    acc[p]+=vv
    accn[p]+=1   
\end{lstlisting}
Dann ermittle ich die Fitparameter fuer eine Gerade and die ersten
60\% der Intensitaeten.
\begin{lstlisting}[style=mypython]
ax=x[nonzero(accn)]
ay=acc/accn
ay=ay[nonzero(accn)]
l=round(.6*len(ax))
bx=ax[0:l]
by=ay[0:l]
plot(ax,ay,'r+')
slope,intercept,rval,pval,stderr=stats.linregress(bx,by)
\end{lstlisting}
Aus dem Anstieg der Kurve bestimme ich den Umrechnungsfaktor von den
Geraeteabhaengigen ADU und kann damit aus der Varianz der Dunkelbilder
das Ausleserauschen als Photoelektronenequivalent angeben.
\begin{lstlisting}[style=mypython]
plot(ax,polyval([slope,intercept],ax))
xlabel('intensity/ADU')
ylabel(r'variance/ADU$^2$')
real_gain=1/slope # unit electrons/ADU
read_noise=sqrt(var(b))*real_gain # electrons RMS per pixel
mean_elecs=(mean(f)-mean(b))*real_gain # photoelectrons electrons per pixel
print gain,cam,real_gain,read_noise,mean_elecs,mean(b),rval,pval,stderr
tit='EM-gain: %s, cam: %s, real gain: %.2f e/ADU\n
read noise: %.2f e RMS/pixel, mean: %.2f e/pixel, offset: %.2f'
% (gain,cam,real_gain,read_noise,mean_elecs,mean(b))
title(tit)
subplot(1,3,2)
imshow(var(b,axis=0))
title('variance of darkimages')
colorbar()
subplot(1,3,3)
imshow(mean(b,axis=0))
title('mean of darkimages')
colorbar()
show()
fig.savefig(cam+'_'+gain+'.png')
\end{lstlisting}


\chapter{Optical sectioning by structured illumination}
\comment{
\jpginput{}{m_phase}{}
}


\begin{figure}[H]
  \centering
  \includegraphics[height=.6\textheight]{m_phase}
  \caption{Structured illumination images of the same of the beads
    from \figref{fig:m_wf}. Of each $z-$slice (rows) four exposures
    with different grating phase (columns) were captured.}
  \label{fig:m_phase}
\end{figure}

\section{HiLo}

In diesem Kapitel simuliere ich ein Verfahren, um optische Schnitte
aus nur zwei Rohbildern zu errechnen. Dafuer nutze ich das
wellenoptische Modell der Bildformung.

Ich werde eine beispielhafte 3D Fluorophorverteilung konstruieren,
die ich anstelle eines Samples verwende. Dann errechne ich eine 3D
point-spread function h (ref 1.13) mit der ich sowohl das Bild auf
der Kamera simulieren kann als auch die 3D Beleuchtungsverteilung im
Sample bei gegebenem Muster auf dem focal plane SLM. Anhand dieser
Daten zeige ich die Errechnung optischer Schnitte mit einer Variante
der HiLo Methode und vergleiche das Ergebnis mit einem einfacherern,
konventionellem Algorithmus.

Fuer die numerische Simulation setze ich die DIPimage Toolbox fuer
Matlab ein. Diese erlaubt es Probleme der Bildverarbeitung mit einer
verhaeltnismaessig eleganten Syntax auszudruecken. 

\cma{syntax of DIPimage} Im folgenden Codelisting fuelle ich
beispielsweise drei dreidimensionale Volumenbilder mit Daten, die eine
Linie, ein Rechteck und eine hohle Kugel mit dickem Rand
darstellen. Die Funktion newim erzeugt ein neues DIPimage Objekt,
dabei verwendet sie entweder explizit angegebene Dimensionen
newim(n,n,n) oder kopiert die Dimensionen von einem anderen Objekt
newim(g). Linie und Rechteck druecke ich einfach mit Array Slicing
Operationen aus, fuer die Kugel nutze ich die Funktion rr(g). Diese
gibt einen 3D array zurueck, in dem jeder Pixel (oder Voxel) den
abstand zum Pixel im Mittelpunkt enthaelt. Die Vergleichsoperationen
ergeben Daten mit Booleschen Datentyp und ich wandle sie implizit
wieder in Zahlen indem ich eine Null addiere.

\cma{Matlab/DIPimage}
\begin{lstlisting}[style=mymatlab]
n = 128;    % number of pixels 
g = newim(n,n,n); % make empty 3d image

%% objects: line, rectangle arranged in two planes and a hollow
%% sphere as a 3d object

lineseg = newim(g);
lineseg(91:91,40:90,floor(n/2)+3) = 1;

rectangle = newim(g);
rectangle(83:114,23:43,floor(n/2)+3) = 1;

hollow_sphere = 0.0 + (30<rr(g) & rr(g)<50); 

S = 12 * lineseg + 4 * rectangle + hollow_sphere;
\end{lstlisting}

Mit dem folgenden Code berechne ich zunaechst die generalisiert
McCutchen Apertur a fuer ein Objektiv mit gegebener numerischer
Apertur. An dieser Stelle wird die Wellenlaenge der Simulation
festgelegt. Diese setze ich hier auf 5 Pixel. Mit 500nm Licht ist
demnach die Pixelpitch unserer Simulation 100nm. 

\cma{on sampling} Der Pixelpitch darf nicht zu gross gewaehlt werden. Ansonsten treten
Artifakte auf und die Simulation wird unrealistische Resultate
liefern. Zu klein sollte man den Pixelpitch jedoch auch nicht
waehlen, weil dann nur ein sehr kleines Feld simuliert wird.
Idealerweise sollte der Pixelpitch so gewaehlt werden, dass das
Torusgebilde der OTF ft(h) gerade nicht den Rand beruehrt. Da der
Torus breiter als hoch ist kann man axial mit geringerer Aufloesung
samplen. Eine korrekte Simulation mit geringstem Aufwand kann mit
dem in (eqref 1.18 eq:resolution) durchgefuehrt werden.  In diesem
Beispielcode verzichte ich jedoch auf diese Optimierung, um die
Lesbarkeit zu erhoehen.


\begin{lstlisting}[style=mymatlab]
%% incoherent 3d point spread function
lambda0 = 5; % vacuum wavelength of the light (in pixels)
ri = 1.52; % refractive index
nu = n/lambda0;  % diameter of the ewald sphere 
NA = 1.4; % numerical aperture of the lens
alpha = asin(NA/n);  % acceptance half-angle of lens

% cut segment from sphere shell as given by numerical aperture
a = q<rr(g) & rr(g)<q+1 & zz(g)>nu*cos(alpha);  

% point spread function in real space
h = abs(ft(a))^2;
\end{lstlisting}

Das Bild auf der Kamera erhaelt man gemaess Gleichung 1.14 durch
Faltung der 3D PSF h und der Fluorophordistribution S. Dabei
beobachtet die Kamera zwar pro Aufnahme nur einen einzelnen Slice,
wenn man einen Stack erzeugt, indem das Sample axial verfahren wird
bekommt man dann jedoch die z-stack Daten in cam.

\begin{lstlisting}[style=mymatlab]
%% image of the 3d fluorophore distribution
cam = convolve(h,S);


%% illumination pattern: 2d grating in one plane of a 3d volume
P = 10      % period of the grating (in pixels)
g = newim(n,n,n);
g(:,:,floor(n/2)) = mod(xx(n,n),P)>floor(P/2);


%% 3d illumination distribution

illum = convolve(h,g);


%% erstelle fokus serie, beleuchtungsebene und bildebene bleiben
%% uebereinander

focus_series = newim(n,n,n,3);
focus_series(:,:,:,0) = convolve(h,shift(S*illum,[0 0 -3]));
focus_series(:,:,:,1) = convolve(h,shift(S*illum,[0 0  0]));
focus_series(:,:,:,2) = convolve(h,shift(S*illum,[0 0  3]));

%% photon shot noise hinzugeben

noise_series = newim(n,n,n,3,2);
noise_series(:,:,:,:,0) = noise(focus_series*10,'poisson');
noise_series(:,:,:,:,1) = noise(focus_series*100,'poisson');
\end{lstlisting}

eigentlich muesste man die periode exakt messen (entweder
fluorescent plane sample oder korrelationsmethode aus kai's paper?)
aber fuer diese simulation ist beleuchtungsperiode P bekannt. wir
verschieben die erste ordnung in die mitte und benutzen einen
tiefpass filter um die anderen ordnungen zu unterdruecken

bemerkung: ueberlapp fuehrt zu artefakten, high res sim, wiener
filter 

\begin{lstlisting}[style=mymatlab]
sigma = 0.07; % filter im fourier raum, bild darstellren
lp = exp(-rr(n,n)^2/(2*sigma^2));
hp = 1-lp;

%% ring entlang der grenzfrequenz des filters fuer die integration

ring = abs(ft(besselj(0,2*pi*sigma*n*rr(n,n))));

normal_ring = ring/sum(abs(ring));

eta = sum(ft(ihp)*normal_ring)/sum(ft(ilp)*normal_ring);
ihilo = eta * ilp + ihp;
\end{lstlisting}


\chapter{Mapping between focal plane SLM and camera}
\label{sec:app_map}
\section{Rigid coordinate transformation}
\label{sec:map_maxima}
The equation for the least squares problem in equation
\ref{eq:rigid-sum} on \pageref{eq:rigid-sum} can be expressed
componentwise:
\begin{align}
  \mathcal{Q}=\sum_i^n&
  \abs{s(\cos\phi r^c_{ix}+q\sin\phi r^c_{iy})+t_x-r^d_{ix}}^2
  +
  \abs{s(-\sin\phi r^c_{ix}+q\cos\phi r^c_{iy})+t_y-r^d_{iy}}^2
\end{align}
The following Maxima code will find the solution to the least squares
problem:
\cma{Maxima}
\begin{lstlisting}[style=mymaxima]
load(minpack)$
q:-1;
g(s,p,tx,ty):=[s*( cos(p)*<cx>+q*sin(p)*<cy>)+tx-<dx>,
               s*(-sin(p)*<cx>+q*cos(p)*<cy>)+ty-<dy>, ... ]$
minpack_lsquares(g(s,p,tx,ty), [s,p,tx,ty], [0.88,-3.1,1200,-20]);
\end{lstlisting}
Where \verb!g! is a vector function, which contains two entries for
each pair of points of the focal plane SLM and camera coordinates.  I
computationally construct the lines according to the given pattern,
replacing \verb!<cx>!, \verb!<cy>!  with measured camera coordinates
and \verb!<dx>!, \verb!<dy>! with display coordinates (see the Matlab
code in the next section).

The function \verb!minpack_lsquares! calls the subroutine \verb!lmder!
which was originally developed for the Fortran package \verb!minpack!.
\cma{Fortran}
\begin{lstlisting}[style=myfortran]
c     subroutine lmder (http://www.netlib.org/minpack/lmder.f)
c     the purpose of lmder is to minimize the sum of the squares of
c     m nonlinear functions in n variables by a modification of
c     the levenberg-marquardt algorithm. the user must provide a
c     subroutine which calculates the functions and the jacobian.
c     the subroutine statement is
c       subroutine lmder(fcn,m,n,x,fvec,fjac,ldfjac,ftol,xtol,gtol,
c                        maxfev,diag,mode,factor,nprint,info,nfev,
c                        njev,ipvt,qtf,wa1,wa2,wa3,wa4)
\end{lstlisting}
Maxima automatically calculates the symbolic Jacobian and thereby
removes an error-prone part of the programmer's work for such
optimization problems. This code could easily be modified for more
complicated transformations, e.g.\ including distortion. Because of
this flexibility, I decided to use Maxima.


\subsection{Application of the rigid transform in OpenGL}
\label{sec:map_opengl}
The results of the parameter optimization can then be used to adjust
the displayed SLM patterns to object positions on earlier camera
images. In particular, it is straight forward, to implement the rigid
transform using OpenGL's transform primitives (OpenGL is the graphics
library, that I usually use).

There are two possibilities of applying the transform: On the one
hand, geometrical primitives might be displayed on the focal plane
SLM, so that particular areas on the camera are illuminated.  On the
other hand, a camera image which was acquired earlier can be displayed
as texture on the focal plane SLM.

Uploading camera images to the focal plane SLM is too slow in our
final system to use the latter method\footnote{However, I obtained
  interesting results with \unit[30]{Hz} frame rate of the camera and
  fast feedback to an LCoS controller that was directly connected to a
  graphics card.}

For this reason, I have mainly used the first method and transform the
geometric primitives before I display them on the focal plane SLM.

Here is the corresponding Common Lisp code to initialize the OpenGL
modelview matrix with the rigid transform, so that drawn objects will
appear at the given positions on the camera.
\cma{Common Lisp}
\begin{lstlisting}[style=mylisp]
(defun load-cam-to-lcos-matrix (&optional (x 0s0) (y 0s0))
  (let* ((s 0.828333873909549) (sx  s)        (sy  (- s))
         (phi -3.102)          (sp (sin phi)) (cp (cos phi))
         (tx 608.433)          (ty 168.918)
         (a (make-array (list 4 4) :element-type 'single-float
             :initial-contents
             (list (list (* sx cp)    (* sy sp)  .0   (+ x tx))
                   (list (* -1 sx sp) (* sy cp)  .0   (+ y ty))
                   (list .0           .0        1.0   .0)
                   (list .0           .0         .0  1.0)))))
    (gl:load-transpose-matrix (sb-ext:array-storage-vector a))))    
\end{lstlisting}  
Alternatively, here is the equivalent code in C (with different
parameters):
\cma{C language}
\begin{lstlisting}[style=myclang]
float m[4*4]; // OpenGL Modelview Matrix
float s=-.8749328910202312,
      sx=s,sy=-s,phi=-.8052030670943575,
      cp=cos(phi),sp=sin(phi),
      tx=1456.71806436377,
      ty=910.4787738693659;
  m[0]=   sx*cp;   m[4]=sy*sp;   m[8] =0;    m[12]=tx; 
  m[1]=-1*sx*sp;   m[5]=sy*cp;   m[9] =0;    m[13]=ty; 
  m[2]=0;          m[6]=0.;      m[10]=1;    m[14]=0;  
  m[3]=0;          m[7]=0.;      m[11]=0;    m[15]=1;  
glMatrixMode(GL_MODELVIEW);
glLoadMatrixf(m);
\end{lstlisting}


\section{Image processing: Localizing bright spots on the camera}
\label{sec:matlab-spots}
Here I show Matlab/DIPimage code \citep{dipimage} to localize
individual spots on the camera images, prepare the input for Maxima,
read back the fitted parameter values and superimpose the transformed
coordinate system of the camera pixels on the the grid of the focal
plane SLM in order to estimate the quality of the fit.

The software development kit of the Andor cameras provides functions
to store image data along with acquisition parameters in the FITS file
format. This can be loaded into Matlab using the function
\verb!readim! from the DIPimage toolbox.

% cd /mnt/scan 

\begin{lstlisting}[style=mymatlab]
%% load the files
% 0 .. 99 spot images
% only 10..99 usable because the first are on border and not illuminated
a = newim(1392,1040,103);
for i=0:102
% Andor's FITS format isn't read correctly correct this by adding 2^15
  a(:,:,i) = 2^15 + readim(sprintf('o%03d.fits',i));
end
\end{lstlisting}
Unfortunately DIPimage's \verb!readim! function seems to have a bug
and loads the data as negative values. I manually correct this.


Of the 103 images, that are loaded into the variable \verb!a!, the
first 100 contain spots and the image at the zero-based index 102 is
the uniformly illuminated image shown in \figref{fig:rigid-pics} left.
The other two images at indices 100 and 101 are two centered disks
with different diameters and are not used here.


FIXME translate In diesem speziellen Experiment, waren die focal plane
spots fuer die ersten zehn Bilder nicht beleuchtet. Daher werden sie
im folgenden nicht weiter analysiert.  Wichtig ist Bild 102. Dieses
enthaelt eine Aufnahme mit uniformer Beleuchtung. Anhand des
Histogramms dieser Aufnahme habe ich den Wert 800 ADU als Grenze fuer
eine Maske festgelegt, die sich nur auf den beleuchteten Bereich
beschraenkt.

Die uniform beleuchtete Aufnahme ist in \figref{fig:rigid-pics} links
dargestellt. Mit ihrer Hilfe normiere ich jede spot-Aufnahme, so dass
die spots in jedem Bild ``corr'' denselben Wert aufweisen unabhaengig
davon, wieviele Lagen von beads sich an der jeweiligen Stelle
befanden. Dabei subtrahiere ich auch einen Hintergrundwert von 510
ADU, den ich aus den unbeleuchteten Pixeln der Bilder abgeleitet habe.
\begin{lstlisting}[style=mymatlab]
bg = 510; 
bright = squeeze(a(:,:,102)); 
mask = gaussf(bright,8) > 800; % create mask with illuminated area

posmax = newim(100,2);
for i = 10:99
  corr = (squeeze(a(:,:,i)) - bg) / bright * mask; % correct for sample non-uniformity
  [coords,vals] = findmaxima(gaussf(corr,32));     % find coordinates of maximum
  [valss,valsind] = sort(vals);    % sort coordinates by intensity
  tmp = coords(valsind,:);         % collect the maximum with highest
  posmax(i,:) = tmp(end,:);        % intensity into result
end
\end{lstlisting}
The DIPimage toolbox provides the function \verb!findmaxima!, that
locates all local maxima in an image with subpixel accuracy. I sort
the result by gray value and only use the largest.  The measured 90
coordinates in \verb!posmax! correspond to $\r^c_i$ in equation
\ref{eq:rigid-sum}.
 
FIXME translate Der folgende Matlab code erzeugt und startet Input
fuer Maxima, wie ich in der vorhergehenden Sektion erlaeutert habe, um
die Transformationsparameter zu ermitteln.

Das automatisch erzeugte Maxima Batch Programm wird dabei in die Datei
\verb!fit.max! gespeichert. Die von Maxima gefundenen Parameter
befindet sich in \verb!max.out!.
\begin{lstlisting}[style=mymatlab]
c = double(posmax)';
cmd = ''; % collect equations in maxima format
for i=10:99 
  dx = num2str(400+50*mod(i,10));
  dy = num2str(500+50*floor(i./10));
  cx = num2str(c(i+1,1));
  cy = num2str(c(i+1,2));
  cmd=[cmd ' s*( cos(p)*' cx '+q*sin(p)*' cy')+tx-' dx ', ...
             s*(-sin(p)*'cx '+q*cos(p)*' cy ')+ty-' dy ','];
end
cmd(:,end) = []; % delete last comma

% load the fitting package and start defining the merit function g
pre = 'load(minpack)$ q:-1; g(s,p,tx,ty):=[';
% now put cmd between
% call the fitting function and store the parameters into max.out
cod = [']$ fit:minpack_lsquares(g(s,p,x,y),[s,p,x,y],[.88,-1.3,1200,-20]);'...
       'write_data(fit[1],"max.out");']
fid = fopen('fit.max','w'); % write maxima commands into file
fwrite(fid,[pre cmd cod]);
fclose(fid);
[max_status,max_result]=system('maxima -b fit.max'); % execute maxima
\end{lstlisting}
I load the transformation parameters back into Matlab and create the a
diagram (shown in \figref{fig:rigid-compare} in the main text on page
\pageref{fig:rigid-compare}) to visualize, how well the transform
matches camera and display coordinates.

\begin{lstlisting} [style=mymatlab]
% load rigid transformation parameters from the file into matlab
params = load('max.out')';
scale  = params(1);
phi    = params(2);
tx     = params(3);
ty     = params(4);
mirr   = -1;
R      = [ cos(phi), mirr*sin(phi); ...
          -sin(phi), mirr*cos(phi)];
T      = [tx ty]';

%% plot the two grids on top of each other
mapped = zeros(100,2);
for i=11:100 % camera coordinates into display coordinates
  mapped(i,:) = (scale*R*q(i,:)'+T)';
end

dpos = zeros(100,2);
for i=0:99 % calculate display points
  dpos(i+1,1) = 400+50*mod(i,10);
  dpos(i+1,2) = 500+50*floor(i./10);
end

hold off; plot(dpos(:,1),dpos(:,2),'.'); hold on;
plot(mapped(11:end,1),mapped(11:end,2),'r+');
\end{lstlisting}

% print -depsc2 /home/martin/thesis/kielhorn/rigid/rigid-compare
